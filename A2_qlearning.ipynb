{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6913f93a",
   "metadata": {},
   "source": [
    "#  Q-Learning on Taxi-v3 — Step-by-Step Student Notebook\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "We are going to teach a virtual taxi driver using Reinforcement Learning (RL).\n",
    "Think of RL like teaching a kid or a pet — we give rewards for good behavior and small punishments for mistakes.\n",
    "The taxi must learn how to pick up and drop off passengers in a small city.\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "- The taxi learns by trial and error.\n",
    "- The world is a small 5x5 grid with pickup and drop-off spots: Red, Green, Yellow, and Blue.\n",
    "- The taxi can move, pick up, and drop off passengers.\n",
    "\n",
    "**Our goal:** train the taxi to earn more rewards and make fewer mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa41ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Problem Definition\n",
    "\n",
    "Right now, our taxi doesn’t know where to go, when to stop, or how to pick up a passenger.\n",
    "\n",
    "We want it to learn the best decisions through experience.\n",
    "\n",
    "For that, we’ll use the Q-Learning algorithm, which helps it remember what worked best in different situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8472d",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Goal and Approach\n",
    "\n",
    "**We will:**\n",
    "\n",
    "- Create the Taxi world using Gymnasium.\n",
    "- Understand its states, actions, and rewards using the helper file assignment2_utils.py.\n",
    "- Build the Q-learning algorithm.\n",
    "- Train it with different learning rates (α) and exploration rates (ε).\n",
    "- Plot results to see how learning improves.\n",
    "- Choose the best model and retrain it.\n",
    "\n",
    "Now let's check and perform test how smart our final taxi has become."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d8e39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b0404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "%pip install gymnasium numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d21bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\habha\\desktop\\reinforcement learning\\assignment 2\\cscn8020_assignment-2_qlearning\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30dfcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from assignment2_utils import describe_env, describe_obs\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0857c",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation \n",
    "\n",
    "- **gymnasium** – builds the taxi environment.\n",
    "- **numpy** – helps with math operations.\n",
    "- **matplotlib** – makes graphs so we can see learning progress.\n",
    "- **pandas**- Makes tables for results.\n",
    "- **assignment2_utils**- Provided helper file that describes actions, observations, and rewards.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ae98e",
   "metadata": {},
   "source": [
    "### **Step 1 : Create and Describe the Taxi World**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff8a987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Discrete(500)\n",
      "Observation space size:  500\n",
      "Reward Range:  (-10, 20)\n",
      "Number of actions:  6\n",
      "Action description:  {0: 'Move south (down)', 1: 'Move north (up)', 2: 'Move east (right)', 3: 'Move west (left)', 4: 'Pickup passenger', 5: 'Drop off passenger'}\n"
     ]
    }
   ],
   "source": [
    "# Create Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Add a compatibility wrapper so env.reward_range exists\n",
    "if not hasattr(env, 'reward_range'):\n",
    "    env.reward_range = (-10, 20)\n",
    "\n",
    "# Now we can safely describe the environment\n",
    "describe_env(env)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ef73d",
   "metadata": {},
   "source": [
    "###  Explanation \n",
    "- There are **500 states** — these are all possible combinations of taxi positions, passenger locations, and destinations.\n",
    "- There are **6 actions** — move south, north, east, west, pick up, and drop off.\n",
    "\n",
    "\n",
    "The helper function describe_env() prints the observation space, reward range, and action meanings — this helps us understand the environment structure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09819685",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Q-Learning\n",
    "Q-learning helps the taxi learn what’s the **best action** in each situation.\n",
    "\n",
    "### The Formula\n",
    "\n",
    "```\n",
    "Q(state, action) = Q(state, action) + α * [reward + γ * max(Q(next_state)) - Q(state, action)]\n",
    "```\n",
    "\n",
    "### Example \n",
    "\n",
    "Imagine you are playing a game:\n",
    "- You jump over an obstacle → +10 points.\n",
    "- You fall → -5 points.\n",
    "Over time, you learn which moves give you the best score. The taxi learns in the same way!\n",
    "\n",
    "Meaning:\n",
    "\n",
    "New Value = Old Value + How Much Better the New Experience Is.\n",
    "\n",
    "α (alpha): learning rate\n",
    "\n",
    "γ (gamma): future importance\n",
    "\n",
    "ε (epsilon): exploration probability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0e3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59defdf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b93db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Code 2: Initialize the Q-Table\n",
    "\n",
    "# Create a Q-table with all zeros\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb5886",
   "metadata": {},
   "source": [
    "###  Explanation \n",
    "\n",
    "- The table has **500 rows (states)** and **6 columns (actions)**.\n",
    "- Each number shows how good an action is in a particular state.\n",
    "- Right now, all are **0** because the taxi hasn’t learned anything yet.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec37d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Code 3: Define Learning Parameters\n",
    "# Set learning parameters\n",
    "alpha = 0.1      # Learning rate (how fast the taxi learns)\n",
    "gamma = 0.9      # Discount factor (how much it values the future)\n",
    "epsilon = 0.1    # Exploration rate (how often it tries random moves)\n",
    "episodes = 1000  # Number of training games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51926cc",
   "metadata": {},
   "source": [
    "###  **Explanation**\n",
    "\n",
    "| Parameter | Meaning | Simple Example |\n",
    "|------------|----------|----------------|\n",
    "| α (alpha) | How fast we learn | Like studying — too fast, we forget old info |\n",
    "| γ (gamma) | Looks at future rewards | Like saving for the future vs spending now |\n",
    "| ε (epsilon) | How often we explore | Trying new routes sometimes helps! |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🏃‍♂️ Code 4: Train the Taxi\n",
    "```python\n",
    "rewards = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Explore vs Exploit decision\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()  # Try something random\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Choose best known move\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Update rule\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "```\n",
    "\n",
    "### 🧠 Explanation (Markdown)\n",
    "- The taxi plays **1000 games (episodes)**.\n",
    "- In each game, it tries moves, gets rewards, and updates the Q-table.\n",
    "- Slowly, it learns what actions lead to the best outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Code 5: Plot the Learning Progress\n",
    "```python\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward per Episode')\n",
    "plt.title('Taxi Learning Over Time')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 📈 Explanation (Markdown)\n",
    "- The line starts low and should go higher as the taxi learns.\n",
    "- A higher reward means the taxi is making better choices.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚖 Code 6: Watch the Trained Taxi\n",
    "```python\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "```\n",
    "\n",
    "### 🎬 Explanation (Markdown)\n",
    "Now the taxi is **smart**! It knows where to go and how to pick up and drop off passengers efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Code 7: Experiment with Parameters\n",
    "```python\n",
    "# Try different learning rates and exploration factors\n",
    "alphas = [0.01, 0.2]\n",
    "gammas = [0.2, 0.3]\n",
    "```\n",
    "\n",
    "### 🧠 Explanation (Markdown)\n",
    "Try changing one value at a time and watch how the learning curve changes.\n",
    "\n",
    "| Parameter | Low Value | High Value | What Happens |\n",
    "|------------|------------|-------------|---------------|\n",
    "| α (alpha) | Learns slowly | Learns too fast | Might forget old things |\n",
    "| γ (gamma) | Focuses on now | Focuses on future | May miss quick wins |\n",
    "| ε (epsilon) | Less exploration | More exploration | More discovery early on |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Conclusion (Markdown)\n",
    "We built and trained a **Q-Learning taxi** that learned to pick up and drop off passengers on its own.\n",
    "\n",
    "### Key Learnings:\n",
    "- Q-Learning uses rewards to guide behavior.\n",
    "- Hyperparameters (α, γ, ε) control how the taxi learns.\n",
    "- The more it practices, the smarter it becomes.\n",
    "\n",
    "🟩 **Real-life Example:** This is how self-driving cars learn — by practicing in simulations before hitting the road!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
